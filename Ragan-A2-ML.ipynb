{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSC478 Machine Learning\n",
    "\n",
    "# Assignment-2: Classification with Logistic Regression and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Connor Ragan AN72374* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've learnt many new topics and concepts in the last 3 chapters (chapter 3, 4, and 5). You learned various performance evaluation strategies that you can use to evaluate your classification and regression models. You also learned that Support Vector Machines (SVM) is an effective algorithm for solving a wide range of ML problems.\n",
    "\n",
    "In Part I of this assignment, you are going to build and compare three classifiers applied on the breast cancer dataset. You will also perform fine-tuning the parameters. As an <font color=\"green\">  extra credit</font> opportunity in Part II, you train a SVM classifier for image classification.\n",
    "\n",
    "Pedagogically, this assignment will help you:\n",
    "- better understand SVM, as well as logistic regression.\n",
    "\n",
    "- how to perform error analysis, parameter settings and model selection.\n",
    "\n",
    "- practice plotting techniques using matplotlib.\n",
    "\n",
    "- pratice reading documentation. This is a very important skill in AI/ML/Data Science collaborative environments and teams.\n",
    "\n",
    "So, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I - Classification with SGD, Logistic Regression, and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Assignment-1, you built a SGD classifier with a good accuracy on the breast cancer dataset. You are now going to build two more classifiers that can be applied on the same datset using the following algorithms: Logistic Regression, and Support Vector Machines (SVM).\n",
    "\n",
    "- You will compare the three classifiers SGD, Logistic Regression and SVM with respect to their parameters and three accuracy measures: f score, auc (ROC), and cross validation.\n",
    "\n",
    "- You will use a grid-search strategy (or brute-force, i.e. trial and error) to fine-tune the parameters of your classifiers.\n",
    "\n",
    "- You will use matplotlib to plot your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Python modules\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the type of the variable `breast_cancer` which holds our data is `Bunch` and is a dictionary-like data structure, we extract features and labels and explore their shape to better understand the data. This version of `breast_cancer` dataset has thirty features which are specifications of the tumor as well as two classes/labels `['malignant' 'benign']`.\n",
    "\n",
    "The data set consists of 569 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "label_names = cancer_data['target_names']\n",
    "labels = cancer_data['target']\n",
    "feature_names = cancer_data['feature_names']\n",
    "features = cancer_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n",
      "0 0\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
      " 1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
      " 6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
      " 1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
      " 4.601e-01 1.189e-01]\n"
     ]
    }
   ],
   "source": [
    "print(label_names)\n",
    "print(labels[0], labels[1])\n",
    "print(feature_names)\n",
    "print(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use pandas to load the data as a dataframe, and then we can use pandas methods to visualize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df = pd.DataFrame(cancer_data.data, columns = cancer_data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.990</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.71190</td>\n",
       "      <td>0.26540</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.570</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.070170</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.24160</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.690</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.45040</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.420</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.68690</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.290</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>0.16250</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.450</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.080890</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>15.470</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.52490</td>\n",
       "      <td>0.53550</td>\n",
       "      <td>0.17410</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.250</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>22.880</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.14420</td>\n",
       "      <td>0.25760</td>\n",
       "      <td>0.37840</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.710</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.059850</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>17.060</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.16540</td>\n",
       "      <td>0.36820</td>\n",
       "      <td>0.26780</td>\n",
       "      <td>0.15560</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.000</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.093530</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>15.490</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.17030</td>\n",
       "      <td>0.54010</td>\n",
       "      <td>0.53900</td>\n",
       "      <td>0.20600</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.460</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.085430</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>...</td>\n",
       "      <td>15.090</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.18530</td>\n",
       "      <td>1.05800</td>\n",
       "      <td>1.10500</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16.020</td>\n",
       "      <td>23.24</td>\n",
       "      <td>102.70</td>\n",
       "      <td>797.8</td>\n",
       "      <td>0.08206</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>0.032990</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>0.05697</td>\n",
       "      <td>...</td>\n",
       "      <td>19.190</td>\n",
       "      <td>33.88</td>\n",
       "      <td>123.80</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.11810</td>\n",
       "      <td>0.15510</td>\n",
       "      <td>0.14590</td>\n",
       "      <td>0.09975</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.08452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15.780</td>\n",
       "      <td>17.89</td>\n",
       "      <td>103.60</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.09710</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.099540</td>\n",
       "      <td>0.066060</td>\n",
       "      <td>0.1842</td>\n",
       "      <td>0.06082</td>\n",
       "      <td>...</td>\n",
       "      <td>20.420</td>\n",
       "      <td>27.28</td>\n",
       "      <td>136.50</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>0.13960</td>\n",
       "      <td>0.56090</td>\n",
       "      <td>0.39650</td>\n",
       "      <td>0.18100</td>\n",
       "      <td>0.3792</td>\n",
       "      <td>0.10480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19.170</td>\n",
       "      <td>24.80</td>\n",
       "      <td>132.40</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>0.09740</td>\n",
       "      <td>0.24580</td>\n",
       "      <td>0.206500</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07800</td>\n",
       "      <td>...</td>\n",
       "      <td>20.960</td>\n",
       "      <td>29.94</td>\n",
       "      <td>151.70</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>0.10370</td>\n",
       "      <td>0.39030</td>\n",
       "      <td>0.36390</td>\n",
       "      <td>0.17670</td>\n",
       "      <td>0.3176</td>\n",
       "      <td>0.10230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.850</td>\n",
       "      <td>23.95</td>\n",
       "      <td>103.70</td>\n",
       "      <td>782.7</td>\n",
       "      <td>0.08401</td>\n",
       "      <td>0.10020</td>\n",
       "      <td>0.099380</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.05338</td>\n",
       "      <td>...</td>\n",
       "      <td>16.840</td>\n",
       "      <td>27.66</td>\n",
       "      <td>112.00</td>\n",
       "      <td>876.5</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.19240</td>\n",
       "      <td>0.23220</td>\n",
       "      <td>0.11190</td>\n",
       "      <td>0.2809</td>\n",
       "      <td>0.06287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.730</td>\n",
       "      <td>22.61</td>\n",
       "      <td>93.60</td>\n",
       "      <td>578.3</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.22930</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.080250</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.07682</td>\n",
       "      <td>...</td>\n",
       "      <td>15.030</td>\n",
       "      <td>32.01</td>\n",
       "      <td>108.80</td>\n",
       "      <td>697.7</td>\n",
       "      <td>0.16510</td>\n",
       "      <td>0.77250</td>\n",
       "      <td>0.69430</td>\n",
       "      <td>0.22080</td>\n",
       "      <td>0.3596</td>\n",
       "      <td>0.14310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.540</td>\n",
       "      <td>27.54</td>\n",
       "      <td>96.73</td>\n",
       "      <td>658.8</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.15950</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.073640</td>\n",
       "      <td>0.2303</td>\n",
       "      <td>0.07077</td>\n",
       "      <td>...</td>\n",
       "      <td>17.460</td>\n",
       "      <td>37.13</td>\n",
       "      <td>124.10</td>\n",
       "      <td>943.2</td>\n",
       "      <td>0.16780</td>\n",
       "      <td>0.65770</td>\n",
       "      <td>0.70260</td>\n",
       "      <td>0.17120</td>\n",
       "      <td>0.4218</td>\n",
       "      <td>0.13410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.680</td>\n",
       "      <td>20.13</td>\n",
       "      <td>94.74</td>\n",
       "      <td>684.5</td>\n",
       "      <td>0.09867</td>\n",
       "      <td>0.07200</td>\n",
       "      <td>0.073950</td>\n",
       "      <td>0.052590</td>\n",
       "      <td>0.1586</td>\n",
       "      <td>0.05922</td>\n",
       "      <td>...</td>\n",
       "      <td>19.070</td>\n",
       "      <td>30.88</td>\n",
       "      <td>123.40</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.14640</td>\n",
       "      <td>0.18710</td>\n",
       "      <td>0.29140</td>\n",
       "      <td>0.16090</td>\n",
       "      <td>0.3029</td>\n",
       "      <td>0.08216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.130</td>\n",
       "      <td>20.68</td>\n",
       "      <td>108.10</td>\n",
       "      <td>798.8</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.20220</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.07356</td>\n",
       "      <td>...</td>\n",
       "      <td>20.960</td>\n",
       "      <td>31.48</td>\n",
       "      <td>136.80</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>0.17890</td>\n",
       "      <td>0.42330</td>\n",
       "      <td>0.47840</td>\n",
       "      <td>0.20730</td>\n",
       "      <td>0.3706</td>\n",
       "      <td>0.11420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.810</td>\n",
       "      <td>22.15</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.10270</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.094980</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.05395</td>\n",
       "      <td>...</td>\n",
       "      <td>27.320</td>\n",
       "      <td>30.88</td>\n",
       "      <td>186.80</td>\n",
       "      <td>2398.0</td>\n",
       "      <td>0.15120</td>\n",
       "      <td>0.31500</td>\n",
       "      <td>0.53720</td>\n",
       "      <td>0.23880</td>\n",
       "      <td>0.2768</td>\n",
       "      <td>0.07615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.540</td>\n",
       "      <td>14.36</td>\n",
       "      <td>87.46</td>\n",
       "      <td>566.3</td>\n",
       "      <td>0.09779</td>\n",
       "      <td>0.08129</td>\n",
       "      <td>0.066640</td>\n",
       "      <td>0.047810</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.05766</td>\n",
       "      <td>...</td>\n",
       "      <td>15.110</td>\n",
       "      <td>19.26</td>\n",
       "      <td>99.70</td>\n",
       "      <td>711.2</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.17730</td>\n",
       "      <td>0.23900</td>\n",
       "      <td>0.12880</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>0.07259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.080</td>\n",
       "      <td>15.71</td>\n",
       "      <td>85.63</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.12700</td>\n",
       "      <td>0.045680</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.06811</td>\n",
       "      <td>...</td>\n",
       "      <td>14.500</td>\n",
       "      <td>20.49</td>\n",
       "      <td>96.09</td>\n",
       "      <td>630.5</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.18900</td>\n",
       "      <td>0.07283</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.08183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.504</td>\n",
       "      <td>12.44</td>\n",
       "      <td>60.34</td>\n",
       "      <td>273.9</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>...</td>\n",
       "      <td>10.230</td>\n",
       "      <td>15.66</td>\n",
       "      <td>65.13</td>\n",
       "      <td>314.9</td>\n",
       "      <td>0.13240</td>\n",
       "      <td>0.11480</td>\n",
       "      <td>0.08867</td>\n",
       "      <td>0.06227</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.07773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.340</td>\n",
       "      <td>14.26</td>\n",
       "      <td>102.50</td>\n",
       "      <td>704.4</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.21350</td>\n",
       "      <td>0.207700</td>\n",
       "      <td>0.097560</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.07032</td>\n",
       "      <td>...</td>\n",
       "      <td>18.070</td>\n",
       "      <td>19.08</td>\n",
       "      <td>125.10</td>\n",
       "      <td>980.9</td>\n",
       "      <td>0.13900</td>\n",
       "      <td>0.59540</td>\n",
       "      <td>0.63050</td>\n",
       "      <td>0.23930</td>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.09946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>21.160</td>\n",
       "      <td>23.04</td>\n",
       "      <td>137.20</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>0.09428</td>\n",
       "      <td>0.10220</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.086320</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.05278</td>\n",
       "      <td>...</td>\n",
       "      <td>29.170</td>\n",
       "      <td>35.59</td>\n",
       "      <td>188.00</td>\n",
       "      <td>2615.0</td>\n",
       "      <td>0.14010</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>0.31550</td>\n",
       "      <td>0.20090</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>0.07526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16.650</td>\n",
       "      <td>21.38</td>\n",
       "      <td>110.00</td>\n",
       "      <td>904.6</td>\n",
       "      <td>0.11210</td>\n",
       "      <td>0.14570</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.091700</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>0.06330</td>\n",
       "      <td>...</td>\n",
       "      <td>26.460</td>\n",
       "      <td>31.56</td>\n",
       "      <td>177.00</td>\n",
       "      <td>2215.0</td>\n",
       "      <td>0.18050</td>\n",
       "      <td>0.35780</td>\n",
       "      <td>0.46950</td>\n",
       "      <td>0.20950</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.09564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.140</td>\n",
       "      <td>16.40</td>\n",
       "      <td>116.00</td>\n",
       "      <td>912.7</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.22760</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>0.07413</td>\n",
       "      <td>...</td>\n",
       "      <td>22.250</td>\n",
       "      <td>21.40</td>\n",
       "      <td>152.40</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>0.15450</td>\n",
       "      <td>0.39490</td>\n",
       "      <td>0.38530</td>\n",
       "      <td>0.25500</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>0.10590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.580</td>\n",
       "      <td>21.53</td>\n",
       "      <td>97.41</td>\n",
       "      <td>644.8</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.18680</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.087830</td>\n",
       "      <td>0.2252</td>\n",
       "      <td>0.06924</td>\n",
       "      <td>...</td>\n",
       "      <td>17.620</td>\n",
       "      <td>33.21</td>\n",
       "      <td>122.40</td>\n",
       "      <td>896.9</td>\n",
       "      <td>0.15250</td>\n",
       "      <td>0.66430</td>\n",
       "      <td>0.55390</td>\n",
       "      <td>0.27010</td>\n",
       "      <td>0.4264</td>\n",
       "      <td>0.12750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18.610</td>\n",
       "      <td>20.25</td>\n",
       "      <td>122.10</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>0.09440</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.077310</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>0.05699</td>\n",
       "      <td>...</td>\n",
       "      <td>21.310</td>\n",
       "      <td>27.26</td>\n",
       "      <td>139.90</td>\n",
       "      <td>1403.0</td>\n",
       "      <td>0.13380</td>\n",
       "      <td>0.21170</td>\n",
       "      <td>0.34460</td>\n",
       "      <td>0.14900</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.07421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15.300</td>\n",
       "      <td>25.27</td>\n",
       "      <td>102.40</td>\n",
       "      <td>732.4</td>\n",
       "      <td>0.10820</td>\n",
       "      <td>0.16970</td>\n",
       "      <td>0.168300</td>\n",
       "      <td>0.087510</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.06540</td>\n",
       "      <td>...</td>\n",
       "      <td>20.270</td>\n",
       "      <td>36.71</td>\n",
       "      <td>149.30</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>0.16410</td>\n",
       "      <td>0.61100</td>\n",
       "      <td>0.63350</td>\n",
       "      <td>0.20240</td>\n",
       "      <td>0.4027</td>\n",
       "      <td>0.09876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17.570</td>\n",
       "      <td>15.05</td>\n",
       "      <td>115.00</td>\n",
       "      <td>955.1</td>\n",
       "      <td>0.09847</td>\n",
       "      <td>0.11570</td>\n",
       "      <td>0.098750</td>\n",
       "      <td>0.079530</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.06149</td>\n",
       "      <td>...</td>\n",
       "      <td>20.010</td>\n",
       "      <td>19.52</td>\n",
       "      <td>134.90</td>\n",
       "      <td>1227.0</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.28120</td>\n",
       "      <td>0.24890</td>\n",
       "      <td>0.14560</td>\n",
       "      <td>0.2756</td>\n",
       "      <td>0.07919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>7.691</td>\n",
       "      <td>25.44</td>\n",
       "      <td>48.34</td>\n",
       "      <td>170.4</td>\n",
       "      <td>0.08668</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.092520</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>0.2037</td>\n",
       "      <td>0.07751</td>\n",
       "      <td>...</td>\n",
       "      <td>8.678</td>\n",
       "      <td>31.89</td>\n",
       "      <td>54.49</td>\n",
       "      <td>223.6</td>\n",
       "      <td>0.15960</td>\n",
       "      <td>0.30640</td>\n",
       "      <td>0.33930</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.10660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>11.540</td>\n",
       "      <td>14.44</td>\n",
       "      <td>74.65</td>\n",
       "      <td>402.9</td>\n",
       "      <td>0.09984</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.067370</td>\n",
       "      <td>0.025940</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0.06782</td>\n",
       "      <td>...</td>\n",
       "      <td>12.260</td>\n",
       "      <td>19.68</td>\n",
       "      <td>78.78</td>\n",
       "      <td>457.8</td>\n",
       "      <td>0.13450</td>\n",
       "      <td>0.21180</td>\n",
       "      <td>0.17970</td>\n",
       "      <td>0.06918</td>\n",
       "      <td>0.2329</td>\n",
       "      <td>0.08134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>14.470</td>\n",
       "      <td>24.99</td>\n",
       "      <td>95.81</td>\n",
       "      <td>656.4</td>\n",
       "      <td>0.08837</td>\n",
       "      <td>0.12300</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.1872</td>\n",
       "      <td>0.06341</td>\n",
       "      <td>...</td>\n",
       "      <td>16.220</td>\n",
       "      <td>31.73</td>\n",
       "      <td>113.50</td>\n",
       "      <td>808.9</td>\n",
       "      <td>0.13400</td>\n",
       "      <td>0.42020</td>\n",
       "      <td>0.40400</td>\n",
       "      <td>0.12050</td>\n",
       "      <td>0.3187</td>\n",
       "      <td>0.10230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>14.740</td>\n",
       "      <td>25.42</td>\n",
       "      <td>94.70</td>\n",
       "      <td>668.6</td>\n",
       "      <td>0.08275</td>\n",
       "      <td>0.07214</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.030270</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.05680</td>\n",
       "      <td>...</td>\n",
       "      <td>16.510</td>\n",
       "      <td>32.29</td>\n",
       "      <td>107.40</td>\n",
       "      <td>826.4</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.13760</td>\n",
       "      <td>0.16110</td>\n",
       "      <td>0.10950</td>\n",
       "      <td>0.2722</td>\n",
       "      <td>0.06956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>13.210</td>\n",
       "      <td>28.06</td>\n",
       "      <td>84.88</td>\n",
       "      <td>538.4</td>\n",
       "      <td>0.08671</td>\n",
       "      <td>0.06877</td>\n",
       "      <td>0.029870</td>\n",
       "      <td>0.032750</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.05781</td>\n",
       "      <td>...</td>\n",
       "      <td>14.370</td>\n",
       "      <td>37.17</td>\n",
       "      <td>92.48</td>\n",
       "      <td>629.6</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0.13810</td>\n",
       "      <td>0.10620</td>\n",
       "      <td>0.07958</td>\n",
       "      <td>0.2473</td>\n",
       "      <td>0.06443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>13.870</td>\n",
       "      <td>20.70</td>\n",
       "      <td>89.77</td>\n",
       "      <td>584.8</td>\n",
       "      <td>0.09578</td>\n",
       "      <td>0.10180</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.023690</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>0.06688</td>\n",
       "      <td>...</td>\n",
       "      <td>15.050</td>\n",
       "      <td>24.75</td>\n",
       "      <td>99.17</td>\n",
       "      <td>688.6</td>\n",
       "      <td>0.12640</td>\n",
       "      <td>0.20370</td>\n",
       "      <td>0.13770</td>\n",
       "      <td>0.06845</td>\n",
       "      <td>0.2249</td>\n",
       "      <td>0.08492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>13.620</td>\n",
       "      <td>23.23</td>\n",
       "      <td>87.19</td>\n",
       "      <td>573.2</td>\n",
       "      <td>0.09246</td>\n",
       "      <td>0.06747</td>\n",
       "      <td>0.029740</td>\n",
       "      <td>0.024430</td>\n",
       "      <td>0.1664</td>\n",
       "      <td>0.05801</td>\n",
       "      <td>...</td>\n",
       "      <td>15.350</td>\n",
       "      <td>29.09</td>\n",
       "      <td>97.58</td>\n",
       "      <td>729.8</td>\n",
       "      <td>0.12160</td>\n",
       "      <td>0.15170</td>\n",
       "      <td>0.10490</td>\n",
       "      <td>0.07174</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>0.06953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>10.320</td>\n",
       "      <td>16.35</td>\n",
       "      <td>65.31</td>\n",
       "      <td>324.9</td>\n",
       "      <td>0.09434</td>\n",
       "      <td>0.04994</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.06201</td>\n",
       "      <td>...</td>\n",
       "      <td>11.250</td>\n",
       "      <td>21.77</td>\n",
       "      <td>71.12</td>\n",
       "      <td>384.9</td>\n",
       "      <td>0.12850</td>\n",
       "      <td>0.08842</td>\n",
       "      <td>0.04384</td>\n",
       "      <td>0.02381</td>\n",
       "      <td>0.2681</td>\n",
       "      <td>0.07399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>10.260</td>\n",
       "      <td>16.58</td>\n",
       "      <td>65.85</td>\n",
       "      <td>320.8</td>\n",
       "      <td>0.08877</td>\n",
       "      <td>0.08066</td>\n",
       "      <td>0.043580</td>\n",
       "      <td>0.024380</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>0.06714</td>\n",
       "      <td>...</td>\n",
       "      <td>10.830</td>\n",
       "      <td>22.04</td>\n",
       "      <td>71.08</td>\n",
       "      <td>357.4</td>\n",
       "      <td>0.14610</td>\n",
       "      <td>0.22460</td>\n",
       "      <td>0.17830</td>\n",
       "      <td>0.08333</td>\n",
       "      <td>0.2691</td>\n",
       "      <td>0.09479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>9.683</td>\n",
       "      <td>19.34</td>\n",
       "      <td>61.05</td>\n",
       "      <td>285.7</td>\n",
       "      <td>0.08491</td>\n",
       "      <td>0.05030</td>\n",
       "      <td>0.023370</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.06235</td>\n",
       "      <td>...</td>\n",
       "      <td>10.930</td>\n",
       "      <td>25.59</td>\n",
       "      <td>69.10</td>\n",
       "      <td>364.2</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.09546</td>\n",
       "      <td>0.09350</td>\n",
       "      <td>0.03846</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>0.07920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>10.820</td>\n",
       "      <td>24.21</td>\n",
       "      <td>68.89</td>\n",
       "      <td>361.6</td>\n",
       "      <td>0.08192</td>\n",
       "      <td>0.06602</td>\n",
       "      <td>0.015480</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>0.1976</td>\n",
       "      <td>0.06328</td>\n",
       "      <td>...</td>\n",
       "      <td>13.030</td>\n",
       "      <td>31.45</td>\n",
       "      <td>83.90</td>\n",
       "      <td>505.6</td>\n",
       "      <td>0.12040</td>\n",
       "      <td>0.16330</td>\n",
       "      <td>0.06194</td>\n",
       "      <td>0.03264</td>\n",
       "      <td>0.3059</td>\n",
       "      <td>0.07626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>10.860</td>\n",
       "      <td>21.48</td>\n",
       "      <td>68.51</td>\n",
       "      <td>360.5</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.04227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1661</td>\n",
       "      <td>0.05948</td>\n",
       "      <td>...</td>\n",
       "      <td>11.660</td>\n",
       "      <td>24.77</td>\n",
       "      <td>74.08</td>\n",
       "      <td>412.3</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.07348</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>0.06592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>11.130</td>\n",
       "      <td>22.44</td>\n",
       "      <td>71.49</td>\n",
       "      <td>378.4</td>\n",
       "      <td>0.09566</td>\n",
       "      <td>0.08194</td>\n",
       "      <td>0.048240</td>\n",
       "      <td>0.022570</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.06552</td>\n",
       "      <td>...</td>\n",
       "      <td>12.020</td>\n",
       "      <td>28.26</td>\n",
       "      <td>77.80</td>\n",
       "      <td>436.6</td>\n",
       "      <td>0.10870</td>\n",
       "      <td>0.17820</td>\n",
       "      <td>0.15640</td>\n",
       "      <td>0.06413</td>\n",
       "      <td>0.3169</td>\n",
       "      <td>0.08032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>12.770</td>\n",
       "      <td>29.43</td>\n",
       "      <td>81.35</td>\n",
       "      <td>507.9</td>\n",
       "      <td>0.08276</td>\n",
       "      <td>0.04234</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>0.014990</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.05637</td>\n",
       "      <td>...</td>\n",
       "      <td>13.870</td>\n",
       "      <td>36.00</td>\n",
       "      <td>88.10</td>\n",
       "      <td>594.7</td>\n",
       "      <td>0.12340</td>\n",
       "      <td>0.10640</td>\n",
       "      <td>0.08653</td>\n",
       "      <td>0.06498</td>\n",
       "      <td>0.2407</td>\n",
       "      <td>0.06484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>9.333</td>\n",
       "      <td>21.94</td>\n",
       "      <td>59.01</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0.09240</td>\n",
       "      <td>0.05605</td>\n",
       "      <td>0.039960</td>\n",
       "      <td>0.012820</td>\n",
       "      <td>0.1692</td>\n",
       "      <td>0.06576</td>\n",
       "      <td>...</td>\n",
       "      <td>9.845</td>\n",
       "      <td>25.05</td>\n",
       "      <td>62.86</td>\n",
       "      <td>295.8</td>\n",
       "      <td>0.11030</td>\n",
       "      <td>0.08298</td>\n",
       "      <td>0.07993</td>\n",
       "      <td>0.02564</td>\n",
       "      <td>0.2435</td>\n",
       "      <td>0.07393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>12.880</td>\n",
       "      <td>28.92</td>\n",
       "      <td>82.50</td>\n",
       "      <td>514.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.05824</td>\n",
       "      <td>0.061950</td>\n",
       "      <td>0.023430</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05708</td>\n",
       "      <td>...</td>\n",
       "      <td>13.890</td>\n",
       "      <td>35.74</td>\n",
       "      <td>88.84</td>\n",
       "      <td>595.7</td>\n",
       "      <td>0.12270</td>\n",
       "      <td>0.16200</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.06493</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>0.07242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>10.290</td>\n",
       "      <td>27.61</td>\n",
       "      <td>65.67</td>\n",
       "      <td>321.4</td>\n",
       "      <td>0.09030</td>\n",
       "      <td>0.07658</td>\n",
       "      <td>0.059990</td>\n",
       "      <td>0.027380</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.06127</td>\n",
       "      <td>...</td>\n",
       "      <td>10.840</td>\n",
       "      <td>34.91</td>\n",
       "      <td>69.57</td>\n",
       "      <td>357.6</td>\n",
       "      <td>0.13840</td>\n",
       "      <td>0.17100</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.09127</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.08283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>10.160</td>\n",
       "      <td>19.59</td>\n",
       "      <td>64.73</td>\n",
       "      <td>311.7</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.07504</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.011160</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.06331</td>\n",
       "      <td>...</td>\n",
       "      <td>10.650</td>\n",
       "      <td>22.88</td>\n",
       "      <td>67.88</td>\n",
       "      <td>347.3</td>\n",
       "      <td>0.12650</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>0.01005</td>\n",
       "      <td>0.02232</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.06742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>9.423</td>\n",
       "      <td>27.88</td>\n",
       "      <td>59.26</td>\n",
       "      <td>271.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.04971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>0.06059</td>\n",
       "      <td>...</td>\n",
       "      <td>10.490</td>\n",
       "      <td>34.24</td>\n",
       "      <td>66.50</td>\n",
       "      <td>330.6</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.07158</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>0.06969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>14.590</td>\n",
       "      <td>22.68</td>\n",
       "      <td>96.39</td>\n",
       "      <td>657.1</td>\n",
       "      <td>0.08473</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.06147</td>\n",
       "      <td>...</td>\n",
       "      <td>15.480</td>\n",
       "      <td>27.27</td>\n",
       "      <td>105.90</td>\n",
       "      <td>733.5</td>\n",
       "      <td>0.10260</td>\n",
       "      <td>0.31710</td>\n",
       "      <td>0.36620</td>\n",
       "      <td>0.11050</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.08004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>11.510</td>\n",
       "      <td>23.93</td>\n",
       "      <td>74.52</td>\n",
       "      <td>403.5</td>\n",
       "      <td>0.09261</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>0.06570</td>\n",
       "      <td>...</td>\n",
       "      <td>12.480</td>\n",
       "      <td>37.16</td>\n",
       "      <td>82.28</td>\n",
       "      <td>474.2</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.25170</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.09653</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>0.08732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>14.050</td>\n",
       "      <td>27.15</td>\n",
       "      <td>91.38</td>\n",
       "      <td>600.4</td>\n",
       "      <td>0.09929</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>0.1537</td>\n",
       "      <td>0.06171</td>\n",
       "      <td>...</td>\n",
       "      <td>15.300</td>\n",
       "      <td>33.17</td>\n",
       "      <td>100.20</td>\n",
       "      <td>706.7</td>\n",
       "      <td>0.12410</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.13260</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.08321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>11.200</td>\n",
       "      <td>29.37</td>\n",
       "      <td>70.67</td>\n",
       "      <td>386.0</td>\n",
       "      <td>0.07449</td>\n",
       "      <td>0.03558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.05502</td>\n",
       "      <td>...</td>\n",
       "      <td>11.920</td>\n",
       "      <td>38.30</td>\n",
       "      <td>75.19</td>\n",
       "      <td>439.6</td>\n",
       "      <td>0.09267</td>\n",
       "      <td>0.05494</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>15.220</td>\n",
       "      <td>30.62</td>\n",
       "      <td>103.40</td>\n",
       "      <td>716.9</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.20870</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.094290</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.07152</td>\n",
       "      <td>...</td>\n",
       "      <td>17.520</td>\n",
       "      <td>42.79</td>\n",
       "      <td>128.70</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0.14170</td>\n",
       "      <td>0.79170</td>\n",
       "      <td>1.17000</td>\n",
       "      <td>0.23560</td>\n",
       "      <td>0.4089</td>\n",
       "      <td>0.14090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>20.920</td>\n",
       "      <td>25.09</td>\n",
       "      <td>143.00</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.22360</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.2149</td>\n",
       "      <td>0.06879</td>\n",
       "      <td>...</td>\n",
       "      <td>24.290</td>\n",
       "      <td>29.41</td>\n",
       "      <td>179.10</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>0.14070</td>\n",
       "      <td>0.41860</td>\n",
       "      <td>0.65990</td>\n",
       "      <td>0.25420</td>\n",
       "      <td>0.2929</td>\n",
       "      <td>0.09873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.560</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.243900</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.41070</td>\n",
       "      <td>0.22160</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.130</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.097910</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.32150</td>\n",
       "      <td>0.16280</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.600</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.092510</td>\n",
       "      <td>0.053020</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.34030</td>\n",
       "      <td>0.14180</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.600</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.93870</td>\n",
       "      <td>0.26500</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.760</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0         17.990         10.38          122.80     1001.0          0.11840   \n",
       "1         20.570         17.77          132.90     1326.0          0.08474   \n",
       "2         19.690         21.25          130.00     1203.0          0.10960   \n",
       "3         11.420         20.38           77.58      386.1          0.14250   \n",
       "4         20.290         14.34          135.10     1297.0          0.10030   \n",
       "5         12.450         15.70           82.57      477.1          0.12780   \n",
       "6         18.250         19.98          119.60     1040.0          0.09463   \n",
       "7         13.710         20.83           90.20      577.9          0.11890   \n",
       "8         13.000         21.82           87.50      519.8          0.12730   \n",
       "9         12.460         24.04           83.97      475.9          0.11860   \n",
       "10        16.020         23.24          102.70      797.8          0.08206   \n",
       "11        15.780         17.89          103.60      781.0          0.09710   \n",
       "12        19.170         24.80          132.40     1123.0          0.09740   \n",
       "13        15.850         23.95          103.70      782.7          0.08401   \n",
       "14        13.730         22.61           93.60      578.3          0.11310   \n",
       "15        14.540         27.54           96.73      658.8          0.11390   \n",
       "16        14.680         20.13           94.74      684.5          0.09867   \n",
       "17        16.130         20.68          108.10      798.8          0.11700   \n",
       "18        19.810         22.15          130.00     1260.0          0.09831   \n",
       "19        13.540         14.36           87.46      566.3          0.09779   \n",
       "20        13.080         15.71           85.63      520.0          0.10750   \n",
       "21         9.504         12.44           60.34      273.9          0.10240   \n",
       "22        15.340         14.26          102.50      704.4          0.10730   \n",
       "23        21.160         23.04          137.20     1404.0          0.09428   \n",
       "24        16.650         21.38          110.00      904.6          0.11210   \n",
       "25        17.140         16.40          116.00      912.7          0.11860   \n",
       "26        14.580         21.53           97.41      644.8          0.10540   \n",
       "27        18.610         20.25          122.10     1094.0          0.09440   \n",
       "28        15.300         25.27          102.40      732.4          0.10820   \n",
       "29        17.570         15.05          115.00      955.1          0.09847   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "539        7.691         25.44           48.34      170.4          0.08668   \n",
       "540       11.540         14.44           74.65      402.9          0.09984   \n",
       "541       14.470         24.99           95.81      656.4          0.08837   \n",
       "542       14.740         25.42           94.70      668.6          0.08275   \n",
       "543       13.210         28.06           84.88      538.4          0.08671   \n",
       "544       13.870         20.70           89.77      584.8          0.09578   \n",
       "545       13.620         23.23           87.19      573.2          0.09246   \n",
       "546       10.320         16.35           65.31      324.9          0.09434   \n",
       "547       10.260         16.58           65.85      320.8          0.08877   \n",
       "548        9.683         19.34           61.05      285.7          0.08491   \n",
       "549       10.820         24.21           68.89      361.6          0.08192   \n",
       "550       10.860         21.48           68.51      360.5          0.07431   \n",
       "551       11.130         22.44           71.49      378.4          0.09566   \n",
       "552       12.770         29.43           81.35      507.9          0.08276   \n",
       "553        9.333         21.94           59.01      264.0          0.09240   \n",
       "554       12.880         28.92           82.50      514.3          0.08123   \n",
       "555       10.290         27.61           65.67      321.4          0.09030   \n",
       "556       10.160         19.59           64.73      311.7          0.10030   \n",
       "557        9.423         27.88           59.26      271.3          0.08123   \n",
       "558       14.590         22.68           96.39      657.1          0.08473   \n",
       "559       11.510         23.93           74.52      403.5          0.09261   \n",
       "560       14.050         27.15           91.38      600.4          0.09929   \n",
       "561       11.200         29.37           70.67      386.0          0.07449   \n",
       "562       15.220         30.62          103.40      716.9          0.10480   \n",
       "563       20.920         25.09          143.00     1347.0          0.10990   \n",
       "564       21.560         22.39          142.00     1479.0          0.11100   \n",
       "565       20.130         28.25          131.20     1261.0          0.09780   \n",
       "566       16.600         28.08          108.30      858.1          0.08455   \n",
       "567       20.600         29.33          140.10     1265.0          0.11780   \n",
       "568        7.760         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760        0.300100             0.147100         0.2419   \n",
       "1             0.07864        0.086900             0.070170         0.1812   \n",
       "2             0.15990        0.197400             0.127900         0.2069   \n",
       "3             0.28390        0.241400             0.105200         0.2597   \n",
       "4             0.13280        0.198000             0.104300         0.1809   \n",
       "5             0.17000        0.157800             0.080890         0.2087   \n",
       "6             0.10900        0.112700             0.074000         0.1794   \n",
       "7             0.16450        0.093660             0.059850         0.2196   \n",
       "8             0.19320        0.185900             0.093530         0.2350   \n",
       "9             0.23960        0.227300             0.085430         0.2030   \n",
       "10            0.06669        0.032990             0.033230         0.1528   \n",
       "11            0.12920        0.099540             0.066060         0.1842   \n",
       "12            0.24580        0.206500             0.111800         0.2397   \n",
       "13            0.10020        0.099380             0.053640         0.1847   \n",
       "14            0.22930        0.212800             0.080250         0.2069   \n",
       "15            0.15950        0.163900             0.073640         0.2303   \n",
       "16            0.07200        0.073950             0.052590         0.1586   \n",
       "17            0.20220        0.172200             0.102800         0.2164   \n",
       "18            0.10270        0.147900             0.094980         0.1582   \n",
       "19            0.08129        0.066640             0.047810         0.1885   \n",
       "20            0.12700        0.045680             0.031100         0.1967   \n",
       "21            0.06492        0.029560             0.020760         0.1815   \n",
       "22            0.21350        0.207700             0.097560         0.2521   \n",
       "23            0.10220        0.109700             0.086320         0.1769   \n",
       "24            0.14570        0.152500             0.091700         0.1995   \n",
       "25            0.22760        0.222900             0.140100         0.3040   \n",
       "26            0.18680        0.142500             0.087830         0.2252   \n",
       "27            0.10660        0.149000             0.077310         0.1697   \n",
       "28            0.16970        0.168300             0.087510         0.1926   \n",
       "29            0.11570        0.098750             0.079530         0.1739   \n",
       "..                ...             ...                  ...            ...   \n",
       "539           0.11990        0.092520             0.013640         0.2037   \n",
       "540           0.11200        0.067370             0.025940         0.1818   \n",
       "541           0.12300        0.100900             0.038900         0.1872   \n",
       "542           0.07214        0.041050             0.030270         0.1840   \n",
       "543           0.06877        0.029870             0.032750         0.1628   \n",
       "544           0.10180        0.036880             0.023690         0.1620   \n",
       "545           0.06747        0.029740             0.024430         0.1664   \n",
       "546           0.04994        0.010120             0.005495         0.1885   \n",
       "547           0.08066        0.043580             0.024380         0.1669   \n",
       "548           0.05030        0.023370             0.009615         0.1580   \n",
       "549           0.06602        0.015480             0.008160         0.1976   \n",
       "550           0.04227        0.000000             0.000000         0.1661   \n",
       "551           0.08194        0.048240             0.022570         0.2030   \n",
       "552           0.04234        0.019970             0.014990         0.1539   \n",
       "553           0.05605        0.039960             0.012820         0.1692   \n",
       "554           0.05824        0.061950             0.023430         0.1566   \n",
       "555           0.07658        0.059990             0.027380         0.1593   \n",
       "556           0.07504        0.005025             0.011160         0.1791   \n",
       "557           0.04971        0.000000             0.000000         0.1742   \n",
       "558           0.13300        0.102900             0.037360         0.1454   \n",
       "559           0.10210        0.111200             0.041050         0.1388   \n",
       "560           0.11260        0.044620             0.043040         0.1537   \n",
       "561           0.03558        0.000000             0.000000         0.1060   \n",
       "562           0.20870        0.255000             0.094290         0.2128   \n",
       "563           0.22360        0.317400             0.147400         0.2149   \n",
       "564           0.11590        0.243900             0.138900         0.1726   \n",
       "565           0.10340        0.144000             0.097910         0.1752   \n",
       "566           0.10230        0.092510             0.053020         0.1590   \n",
       "567           0.27700        0.351400             0.152000         0.2397   \n",
       "568           0.04362        0.000000             0.000000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "5                   0.07613  ...        15.470          23.75   \n",
       "6                   0.05742  ...        22.880          27.66   \n",
       "7                   0.07451  ...        17.060          28.14   \n",
       "8                   0.07389  ...        15.490          30.73   \n",
       "9                   0.08243  ...        15.090          40.68   \n",
       "10                  0.05697  ...        19.190          33.88   \n",
       "11                  0.06082  ...        20.420          27.28   \n",
       "12                  0.07800  ...        20.960          29.94   \n",
       "13                  0.05338  ...        16.840          27.66   \n",
       "14                  0.07682  ...        15.030          32.01   \n",
       "15                  0.07077  ...        17.460          37.13   \n",
       "16                  0.05922  ...        19.070          30.88   \n",
       "17                  0.07356  ...        20.960          31.48   \n",
       "18                  0.05395  ...        27.320          30.88   \n",
       "19                  0.05766  ...        15.110          19.26   \n",
       "20                  0.06811  ...        14.500          20.49   \n",
       "21                  0.06905  ...        10.230          15.66   \n",
       "22                  0.07032  ...        18.070          19.08   \n",
       "23                  0.05278  ...        29.170          35.59   \n",
       "24                  0.06330  ...        26.460          31.56   \n",
       "25                  0.07413  ...        22.250          21.40   \n",
       "26                  0.06924  ...        17.620          33.21   \n",
       "27                  0.05699  ...        21.310          27.26   \n",
       "28                  0.06540  ...        20.270          36.71   \n",
       "29                  0.06149  ...        20.010          19.52   \n",
       "..                      ...  ...           ...            ...   \n",
       "539                 0.07751  ...         8.678          31.89   \n",
       "540                 0.06782  ...        12.260          19.68   \n",
       "541                 0.06341  ...        16.220          31.73   \n",
       "542                 0.05680  ...        16.510          32.29   \n",
       "543                 0.05781  ...        14.370          37.17   \n",
       "544                 0.06688  ...        15.050          24.75   \n",
       "545                 0.05801  ...        15.350          29.09   \n",
       "546                 0.06201  ...        11.250          21.77   \n",
       "547                 0.06714  ...        10.830          22.04   \n",
       "548                 0.06235  ...        10.930          25.59   \n",
       "549                 0.06328  ...        13.030          31.45   \n",
       "550                 0.05948  ...        11.660          24.77   \n",
       "551                 0.06552  ...        12.020          28.26   \n",
       "552                 0.05637  ...        13.870          36.00   \n",
       "553                 0.06576  ...         9.845          25.05   \n",
       "554                 0.05708  ...        13.890          35.74   \n",
       "555                 0.06127  ...        10.840          34.91   \n",
       "556                 0.06331  ...        10.650          22.88   \n",
       "557                 0.06059  ...        10.490          34.24   \n",
       "558                 0.06147  ...        15.480          27.27   \n",
       "559                 0.06570  ...        12.480          37.16   \n",
       "560                 0.06171  ...        15.300          33.17   \n",
       "561                 0.05502  ...        11.920          38.30   \n",
       "562                 0.07152  ...        17.520          42.79   \n",
       "563                 0.06879  ...        24.290          29.41   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "5             103.40       741.6           0.17910            0.52490   \n",
       "6             153.20      1606.0           0.14420            0.25760   \n",
       "7             110.60       897.0           0.16540            0.36820   \n",
       "8             106.20       739.3           0.17030            0.54010   \n",
       "9              97.65       711.4           0.18530            1.05800   \n",
       "10            123.80      1150.0           0.11810            0.15510   \n",
       "11            136.50      1299.0           0.13960            0.56090   \n",
       "12            151.70      1332.0           0.10370            0.39030   \n",
       "13            112.00       876.5           0.11310            0.19240   \n",
       "14            108.80       697.7           0.16510            0.77250   \n",
       "15            124.10       943.2           0.16780            0.65770   \n",
       "16            123.40      1138.0           0.14640            0.18710   \n",
       "17            136.80      1315.0           0.17890            0.42330   \n",
       "18            186.80      2398.0           0.15120            0.31500   \n",
       "19             99.70       711.2           0.14400            0.17730   \n",
       "20             96.09       630.5           0.13120            0.27760   \n",
       "21             65.13       314.9           0.13240            0.11480   \n",
       "22            125.10       980.9           0.13900            0.59540   \n",
       "23            188.00      2615.0           0.14010            0.26000   \n",
       "24            177.00      2215.0           0.18050            0.35780   \n",
       "25            152.40      1461.0           0.15450            0.39490   \n",
       "26            122.40       896.9           0.15250            0.66430   \n",
       "27            139.90      1403.0           0.13380            0.21170   \n",
       "28            149.30      1269.0           0.16410            0.61100   \n",
       "29            134.90      1227.0           0.12550            0.28120   \n",
       "..               ...         ...               ...                ...   \n",
       "539            54.49       223.6           0.15960            0.30640   \n",
       "540            78.78       457.8           0.13450            0.21180   \n",
       "541           113.50       808.9           0.13400            0.42020   \n",
       "542           107.40       826.4           0.10600            0.13760   \n",
       "543            92.48       629.6           0.10720            0.13810   \n",
       "544            99.17       688.6           0.12640            0.20370   \n",
       "545            97.58       729.8           0.12160            0.15170   \n",
       "546            71.12       384.9           0.12850            0.08842   \n",
       "547            71.08       357.4           0.14610            0.22460   \n",
       "548            69.10       364.2           0.11990            0.09546   \n",
       "549            83.90       505.6           0.12040            0.16330   \n",
       "550            74.08       412.3           0.10010            0.07348   \n",
       "551            77.80       436.6           0.10870            0.17820   \n",
       "552            88.10       594.7           0.12340            0.10640   \n",
       "553            62.86       295.8           0.11030            0.08298   \n",
       "554            88.84       595.7           0.12270            0.16200   \n",
       "555            69.57       357.6           0.13840            0.17100   \n",
       "556            67.88       347.3           0.12650            0.12000   \n",
       "557            66.50       330.6           0.10730            0.07158   \n",
       "558           105.90       733.5           0.10260            0.31710   \n",
       "559            82.28       474.2           0.12980            0.25170   \n",
       "560           100.20       706.7           0.12410            0.22640   \n",
       "561            75.19       439.6           0.09267            0.05494   \n",
       "562           128.70       915.0           0.14170            0.79170   \n",
       "563           179.10      1819.0           0.14070            0.41860   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0            0.71190               0.26540          0.4601   \n",
       "1            0.24160               0.18600          0.2750   \n",
       "2            0.45040               0.24300          0.3613   \n",
       "3            0.68690               0.25750          0.6638   \n",
       "4            0.40000               0.16250          0.2364   \n",
       "5            0.53550               0.17410          0.3985   \n",
       "6            0.37840               0.19320          0.3063   \n",
       "7            0.26780               0.15560          0.3196   \n",
       "8            0.53900               0.20600          0.4378   \n",
       "9            1.10500               0.22100          0.4366   \n",
       "10           0.14590               0.09975          0.2948   \n",
       "11           0.39650               0.18100          0.3792   \n",
       "12           0.36390               0.17670          0.3176   \n",
       "13           0.23220               0.11190          0.2809   \n",
       "14           0.69430               0.22080          0.3596   \n",
       "15           0.70260               0.17120          0.4218   \n",
       "16           0.29140               0.16090          0.3029   \n",
       "17           0.47840               0.20730          0.3706   \n",
       "18           0.53720               0.23880          0.2768   \n",
       "19           0.23900               0.12880          0.2977   \n",
       "20           0.18900               0.07283          0.3184   \n",
       "21           0.08867               0.06227          0.2450   \n",
       "22           0.63050               0.23930          0.4667   \n",
       "23           0.31550               0.20090          0.2822   \n",
       "24           0.46950               0.20950          0.3613   \n",
       "25           0.38530               0.25500          0.4066   \n",
       "26           0.55390               0.27010          0.4264   \n",
       "27           0.34460               0.14900          0.2341   \n",
       "28           0.63350               0.20240          0.4027   \n",
       "29           0.24890               0.14560          0.2756   \n",
       "..               ...                   ...             ...   \n",
       "539          0.33930               0.05000          0.2790   \n",
       "540          0.17970               0.06918          0.2329   \n",
       "541          0.40400               0.12050          0.3187   \n",
       "542          0.16110               0.10950          0.2722   \n",
       "543          0.10620               0.07958          0.2473   \n",
       "544          0.13770               0.06845          0.2249   \n",
       "545          0.10490               0.07174          0.2642   \n",
       "546          0.04384               0.02381          0.2681   \n",
       "547          0.17830               0.08333          0.2691   \n",
       "548          0.09350               0.03846          0.2552   \n",
       "549          0.06194               0.03264          0.3059   \n",
       "550          0.00000               0.00000          0.2458   \n",
       "551          0.15640               0.06413          0.3169   \n",
       "552          0.08653               0.06498          0.2407   \n",
       "553          0.07993               0.02564          0.2435   \n",
       "554          0.24390               0.06493          0.2372   \n",
       "555          0.20000               0.09127          0.2226   \n",
       "556          0.01005               0.02232          0.2262   \n",
       "557          0.00000               0.00000          0.2475   \n",
       "558          0.36620               0.11050          0.2258   \n",
       "559          0.36300               0.09653          0.2112   \n",
       "560          0.13260               0.10480          0.2250   \n",
       "561          0.00000               0.00000          0.1566   \n",
       "562          1.17000               0.23560          0.4089   \n",
       "563          0.65990               0.25420          0.2929   \n",
       "564          0.41070               0.22160          0.2060   \n",
       "565          0.32150               0.16280          0.2572   \n",
       "566          0.34030               0.14180          0.2218   \n",
       "567          0.93870               0.26500          0.4087   \n",
       "568          0.00000               0.00000          0.2871   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "5                    0.12440  \n",
       "6                    0.08368  \n",
       "7                    0.11510  \n",
       "8                    0.10720  \n",
       "9                    0.20750  \n",
       "10                   0.08452  \n",
       "11                   0.10480  \n",
       "12                   0.10230  \n",
       "13                   0.06287  \n",
       "14                   0.14310  \n",
       "15                   0.13410  \n",
       "16                   0.08216  \n",
       "17                   0.11420  \n",
       "18                   0.07615  \n",
       "19                   0.07259  \n",
       "20                   0.08183  \n",
       "21                   0.07773  \n",
       "22                   0.09946  \n",
       "23                   0.07526  \n",
       "24                   0.09564  \n",
       "25                   0.10590  \n",
       "26                   0.12750  \n",
       "27                   0.07421  \n",
       "28                   0.09876  \n",
       "29                   0.07919  \n",
       "..                       ...  \n",
       "539                  0.10660  \n",
       "540                  0.08134  \n",
       "541                  0.10230  \n",
       "542                  0.06956  \n",
       "543                  0.06443  \n",
       "544                  0.08492  \n",
       "545                  0.06953  \n",
       "546                  0.07399  \n",
       "547                  0.09479  \n",
       "548                  0.07920  \n",
       "549                  0.07626  \n",
       "550                  0.06592  \n",
       "551                  0.08032  \n",
       "552                  0.06484  \n",
       "553                  0.07393  \n",
       "554                  0.07242  \n",
       "555                  0.08283  \n",
       "556                  0.06742  \n",
       "557                  0.06969  \n",
       "558                  0.08004  \n",
       "559                  0.08732  \n",
       "560                  0.08321  \n",
       "561                  0.05905  \n",
       "562                  0.14090  \n",
       "563                  0.09873  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following cell would plot a histogram of `mean radius` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x21ca4ba4518>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cancer_df['mean radius'].hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following cell would plot a histogram of `mean radius` values across the two different classes with two different colors. Notice that it is a similar histogram, but can tell us the frequency of `mean radius` values across the binary classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for class_number in np.unique(cancer_data.target):\n",
    "    plt.figure(1)\n",
    "    cancer_df['mean radius'].iloc[np.where(cancer_data.target == class_number)[0]].hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, it looks like that `mean radius` is highly correlated with the class labels, there is a clear distinction in two classes in terms of the mean radius. Data visualization often helps us get better insights about the data before we tackle to solve the ML problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can split the data. The split ratio we are going to choose is 0.7 for training and 0.3 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.374e+01 1.791e+01 8.812e+01 ... 6.019e-02 2.350e-01 7.014e-02]\n",
      " [1.337e+01 1.639e+01 8.610e+01 ... 8.978e-02 2.048e-01 7.628e-02]\n",
      " [1.469e+01 1.398e+01 9.822e+01 ... 1.108e-01 2.827e-01 9.208e-02]\n",
      " ...\n",
      " [1.429e+01 1.682e+01 9.030e+01 ... 3.333e-02 2.458e-01 6.120e-02]\n",
      " [1.398e+01 1.962e+01 9.112e+01 ... 1.827e-01 3.179e-01 1.055e-01]\n",
      " [1.218e+01 2.052e+01 7.722e+01 ... 7.431e-02 2.694e-01 6.878e-02]]\n",
      "(171, 30)\n",
      "[1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1\n",
      " 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0\n",
      " 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1\n",
      " 1 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0\n",
      " 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0\n",
      " 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 1 1 0\n",
      " 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1]\n",
      "(171,)\n",
      "[1.374e+01 1.791e+01 8.812e+01 5.850e+02 7.944e-02 6.376e-02 2.881e-02\n",
      " 1.329e-02 1.473e-01 5.580e-02 2.500e-01 7.574e-01 1.573e+00 2.147e+01\n",
      " 2.838e-03 1.592e-02 1.780e-02 5.828e-03 1.329e-02 1.976e-03 1.534e+01\n",
      " 2.246e+01 9.719e+01 7.259e+02 9.711e-02 1.824e-01 1.564e-01 6.019e-02\n",
      " 2.350e-01 7.014e-02]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.30, random_state=42)\n",
    "print(X_train)\n",
    "print(X_test.shape)\n",
    "print(y_train)\n",
    "print(y_test.shape)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we can now choose which classifier from the built-in classifiers in sklearn we want to choose. We are going to use three classifiers: Stochastic Gradient Descent Classifier (SGD), Logist Regression and Support Vector Machines (SVM). We start with SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 9121.41, NNZs: 30, Bias: 172.846213, T: 398, Avg. loss: 741135.454686\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 8359.92, NNZs: 30, Bias: 216.184497, T: 796, Avg. loss: 196476.028283\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7145.00, NNZs: 30, Bias: 237.610356, T: 1194, Avg. loss: 98982.872432\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6790.72, NNZs: 30, Bias: 254.218374, T: 1592, Avg. loss: 77298.994095\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5979.26, NNZs: 30, Bias: 261.883279, T: 1990, Avg. loss: 48556.038617\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 5462.77, NNZs: 30, Bias: 267.899818, T: 2388, Avg. loss: 36934.617011\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 5090.48, NNZs: 30, Bias: 274.474054, T: 2786, Avg. loss: 32621.692247\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 4646.33, NNZs: 30, Bias: 277.922027, T: 3184, Avg. loss: 25389.039541\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 4391.64, NNZs: 30, Bias: 281.553036, T: 3582, Avg. loss: 18111.989108\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 4173.02, NNZs: 30, Bias: 284.593036, T: 3980, Avg. loss: 19944.155169\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 3905.88, NNZs: 30, Bias: 286.674928, T: 4378, Avg. loss: 18494.097893\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 3679.62, NNZs: 30, Bias: 288.568148, T: 4776, Avg. loss: 16646.501188\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 3573.79, NNZs: 30, Bias: 291.465084, T: 5174, Avg. loss: 16089.842103\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 3410.25, NNZs: 30, Bias: 293.084193, T: 5572, Avg. loss: 15430.010003\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 3290.38, NNZs: 30, Bias: 294.953602, T: 5970, Avg. loss: 14088.151085\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 3156.17, NNZs: 30, Bias: 295.899665, T: 6368, Avg. loss: 13576.025152\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 3058.87, NNZs: 30, Bias: 297.226629, T: 6766, Avg. loss: 12289.672884\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2967.87, NNZs: 30, Bias: 298.765796, T: 7164, Avg. loss: 11926.938723\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2869.45, NNZs: 30, Bias: 299.694903, T: 7562, Avg. loss: 9209.894463\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2751.36, NNZs: 30, Bias: 300.204187, T: 7960, Avg. loss: 10269.281220\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2671.01, NNZs: 30, Bias: 301.171262, T: 8358, Avg. loss: 8727.744685\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2601.04, NNZs: 30, Bias: 302.086351, T: 8756, Avg. loss: 8849.466082\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2531.08, NNZs: 30, Bias: 302.743178, T: 9154, Avg. loss: 7196.166461\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2475.17, NNZs: 30, Bias: 303.687357, T: 9552, Avg. loss: 9041.659316\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 2399.10, NNZs: 30, Bias: 304.093790, T: 9950, Avg. loss: 7277.318928\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2358.91, NNZs: 30, Bias: 304.869495, T: 10348, Avg. loss: 6675.638519\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 2297.60, NNZs: 30, Bias: 305.240391, T: 10746, Avg. loss: 8555.346352\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 2243.51, NNZs: 30, Bias: 305.689873, T: 11144, Avg. loss: 5464.758742\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 2160.68, NNZs: 30, Bias: 305.773748, T: 11542, Avg. loss: 6629.641665\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 2121.11, NNZs: 30, Bias: 306.193905, T: 11940, Avg. loss: 5368.508870\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 2089.65, NNZs: 30, Bias: 306.922339, T: 12338, Avg. loss: 5862.396096\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 2034.57, NNZs: 30, Bias: 307.157776, T: 12736, Avg. loss: 5543.143344\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1992.26, NNZs: 30, Bias: 307.538845, T: 13134, Avg. loss: 4750.708791\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1947.04, NNZs: 30, Bias: 307.760887, T: 13532, Avg. loss: 4922.468706\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1898.12, NNZs: 30, Bias: 308.047887, T: 13930, Avg. loss: 5222.987929\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1860.37, NNZs: 30, Bias: 308.396088, T: 14328, Avg. loss: 5793.618492\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1808.27, NNZs: 30, Bias: 308.599262, T: 14726, Avg. loss: 5539.046898\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1775.09, NNZs: 30, Bias: 308.796313, T: 15124, Avg. loss: 4460.769285\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1754.83, NNZs: 30, Bias: 309.249750, T: 15522, Avg. loss: 5719.680417\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1711.12, NNZs: 30, Bias: 309.311466, T: 15920, Avg. loss: 3641.070960\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 1680.53, NNZs: 30, Bias: 309.494666, T: 16318, Avg. loss: 3949.537772\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1650.56, NNZs: 30, Bias: 309.733607, T: 16716, Avg. loss: 3581.648071\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1615.78, NNZs: 30, Bias: 309.850442, T: 17114, Avg. loss: 3853.554686\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1589.42, NNZs: 30, Bias: 310.195093, T: 17512, Avg. loss: 4136.364221\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 1565.86, NNZs: 30, Bias: 310.474486, T: 17910, Avg. loss: 3836.014023\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1546.03, NNZs: 30, Bias: 310.748320, T: 18308, Avg. loss: 3895.577712\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 1510.71, NNZs: 30, Bias: 310.748334, T: 18706, Avg. loss: 4052.729774\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 47 epochs took 0.01 seconds\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "### START CODING HERE ###\n",
    "# Import necessary module from sklearn.linear_model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Call SGDClassifier with a random_state of 42\n",
    "sgd_clf = SGDClassifier(alpha = .001, random_state=42, verbose=1)\n",
    "\n",
    "# Fit the model to X_train and y_train\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on X_test and store the results in y_pred\n",
    "y_pred = sgd_clf.predict(X_test)\n",
    "\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now, evaluate your model using the metrics you've learnt in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.958139534883721\n",
      "auc:  0.9451058201058201\n",
      "cross validation:  0.8695806079261255\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "### START CODING HERE ###\n",
    "# Import the necessary modules from sklearn for f1_score, roc_auc_score and cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Compute the f1_score, roc_auc_score and cross_val_score and store them in properly named variables: \n",
    "# f1_score_sgd, auc_sgd, cv_sgd\n",
    "# Hint: for cross validation with 5 folds, you should call the method on\n",
    "# (sgd_clf, cancer_data.data, cancer_data.target, cv=5)\n",
    "f1_score_sgd = f1_score(y_test, y_pred)\n",
    "auc_sgd = roc_auc_score(y_test, y_pred)\n",
    "cv_sgd = cross_val_score(sgd_clf, cancer_data.data, cancer_data.target, cv=5)\n",
    "### END CODING HERE ###\n",
    "\n",
    "# Print the performance measures\n",
    "print(\"f1_score: \", f1_score_sgd)\n",
    "print(\"auc: \", auc_sgd)\n",
    "print(\"cross validation: \", cv_sgd.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now, let's try logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.9724770642201834\n",
      "auc:  0.958994708994709\n",
      "cross validation:  0.9509041939207385\n"
     ]
    }
   ],
   "source": [
    "### START CODING HERE ###\n",
    "# Import necessary module from sklearn.linear_model for Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Call LogisticRegression with a random_state of 0, solver='liblinear'\n",
    "logreg_clf = LogisticRegression(random_state=0,solver='liblinear')\n",
    "\n",
    "# Fit the model to X_train and y_train\n",
    "logreg_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on X_test and store the results in y_pred\n",
    "y_pred = logreg_clf.predict(X_test)\n",
    "\n",
    "# Compute the f1_score, roc_auc_score and cross_val_score and store them in properly named variables: \n",
    "# f1_score_logreg, auc_logreg, cv_logreg\n",
    "# Hint: for cross validation with 5 folds, you should call the method on\n",
    "# (logreg_clf, cancer_data.data, cancer_data.target, cv=5)\n",
    "f1_score_logreg = f1_score(y_test, y_pred)\n",
    "auc_logreg = roc_auc_score(y_test, y_pred)\n",
    "cv_logreg = cross_val_score(logreg_clf, cancer_data.data, cancer_data.target, cv=5)\n",
    "### END CODING HERE ###\n",
    "\n",
    "# Print the performance measures\n",
    "print(\"f1_score: \", f1_score_logreg)\n",
    "print(\"auc: \", auc_logreg)\n",
    "print(\"cross validation: \", cv_logreg.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Logistic Regression performed better on this dataset so far, although without parameter tuning. Can you improve your reults by tuning the parameters of SGD (any parameter) or parameters of Logistic Regression? You can check the full list of parameters for each classifier in its sklearn documentation page, [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). This kind of parameter settings is a brute-force or grid-search strategy. You need to try changing at least three parameters, but only report the results below if you noticed any improvements higher than 1% in any of the scores.\n",
    "\n",
    "Report your results HERE:\n",
    "\n",
    "Baseline results:\n",
    "\n",
    "    f1_score:  0.9600000000000001\n",
    "    auc:  0.9285714285714286\n",
    "    cross validation:  0.8574066948826472\n",
    "    \n",
    "SGD improved by changing the parameter loss to 'squared_hinge'\n",
    "\n",
    "    f1_score:  0.9677419354838711  <--- improved\n",
    "    auc:  0.9543650793650794  <--- improved\n",
    "    cross validation:  0.8824009234320893  <--- improved\n",
    "    \n",
    "SGD improved by changing the parameter penalty to 'none'\n",
    "\n",
    "    f1_score:  0.9423076923076924\n",
    "    auc:  0.9378306878306878  <--- improved\n",
    "    cross validation:  0.8981454405540592  <--- improved\n",
    "    \n",
    "SGD improved by changing the parameter alpha to .001\n",
    "\n",
    "    f1_score:  0.9468599033816424\n",
    "    auc:  0.9457671957671958  <--- improved\n",
    "    cross validation:  0.8695806079261255  <--- improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.9767441860465117\n",
      "auc:  0.9702380952380952\n",
      "cross validation:  0.9526433243555215\n"
     ]
    }
   ],
   "source": [
    "# Import necessary module from sklearn for svm\n",
    "from sklearn import svm\n",
    "\n",
    "### START CODING HERE ###\n",
    "# Call svm.SVC with kernel='linear', gamma=10, C=10\n",
    "svm_clf = svm.SVC(kernel='linear',gamma=10,C=10)\n",
    "\n",
    "# Fit the model to X_train and y_train\n",
    "svm_clf.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions on X_test and store the results in y_pred\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Compute the f1_score, roc_auc_score and cross_val_score and store them in properly named variables: \n",
    "# f1_score_svm, auc_svm, cv_svm\n",
    "# Hint: for cross validation with 5 folds, you should call the method on\n",
    "# (svm_clf, cancer_data.data, cancer_data.target, cv=5)\n",
    "f1_score_svm = f1_score(y_test,y_pred)\n",
    "auc_svm = roc_auc_score(y_test,y_pred)\n",
    "cv_svm = cross_val_score(svm_clf, cancer_data.data, cancer_data.target, cv=5)\n",
    "### END CODING HERE ###\n",
    "\n",
    "# Print the performance measures\n",
    "print(\"f1_score: \", f1_score_svm)\n",
    "print(\"auc: \", auc_svm)\n",
    "print(\"cross validation: \", cv_svm.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now, change the svm kernel to `'poly'` and `'rbf'` which are non-linear kernels, and see if they improve or hurt the results or have no significant impact. Report your results HERE:\n",
    "\n",
    "Baseline results:\n",
    "\n",
    "    f1_score:  0.9767441860465117\n",
    "    auc:  0.9702380952380952\n",
    "    cross validation:  0.9526433243555215\n",
    "    \n",
    "Poly results (max_iter = 10000):\n",
    "\n",
    "    f1_score:  0.9310344827586207\n",
    "    auc:  0.873015873015873\n",
    "    cross validation:  0.6456944978838015\n",
    "\n",
    "Rbf results:\n",
    "\n",
    "    f1_score:  0.7741935483870968\n",
    "    auc:  0.5\n",
    "    cross validation:  0.6274259330511736\n",
    "    \n",
    "Clearly a linear kernel gives the best results, both poly and rbf hurt the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How about parameter C? Does it have any impact on the performance? Complete the following cell and plot the impact of varying C in range `[1..100]` on f1_score. Once implemented, running this cell may take a few minutes to generate the correct plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18b316c3668>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create two lists for C_values and f_scores\n",
    "C_values = range(1, 100)\n",
    "f_scores = []\n",
    "\n",
    "### START CODING HERE ###\n",
    "# Write a for loop that does the following steps:\n",
    "# iterate over c in C_values\n",
    "for c in C_values:\n",
    "    # call svm model in each iteration passing a linear kernel, gamma=10 and the current c\n",
    "    svm_clf = svm.SVC(kernel='linear',gamma=10,C=c)\n",
    "    # fit the model on X_train and y-train\n",
    "    svm_clf.fit(X_train,y_train)\n",
    "    # predict X_test and store them in y_pred\n",
    "    y_pred = svm_clf.predict(X_test)\n",
    "    # compute f1_score and append it to f_scores\n",
    "    f_scores.append(f1_score(y_test,y_pred))\n",
    "### END CODING HERE ###\n",
    "\n",
    "plt.title('Impact of SVM C Parameter on f1_score')\n",
    "plt.xlabel('C', fontsize=14)\n",
    "plt.ylabel('f1_score', fontsize=14)\n",
    "plt.plot(C_values, f_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following question HERE:\n",
    "\n",
    "Q1- Based on your plot, what ranges/values for C are optimal? What ranges/values may not be so good?\n",
    "\n",
    "The lower range of values (C<20) are better with what looks like around C=5 or C=8 being optimal. C between 22-25, >70, and especially >90 are bad ranges.\n",
    "\n",
    "Q2- What happens to f1_score for C values closer to 100? Compared to C=1, would the SVM street be wider or narrower?\n",
    "\n",
    "For C closer to 100, the f1_score decreases. The SVM street is narrower at these high C values. \n",
    "\n",
    "Q3- Why do you fit the classifier for cross_val_score on the whole dataset instead of `(X_train, y_train)` only?\n",
    "\n",
    "The cross_val_score classifier does K fold cross validation, so it is picking a random fold to be the test fold and training on the others. Since it splits the data into test and train itself, it makes sense to fit the classifier to the entire dataset rather than just the portion we have declared train prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Image Classification with SVM -  <font color=\"green\"> Optional - Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and extract [CIFAR-10 dataset](https://www.cs.utoronto.ca/~kriz/cifar.html). It consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "\n",
    "The required steps in the pipeline of Part II:\n",
    "\n",
    "- 1- Load the image dataset using the provided helper functions in CIFAR website or the functions you write.\n",
    "\n",
    "- 2- Train a linear or non-linear SVM classifier that can classify the images.\n",
    "\n",
    "- 3- Fine-tune your model to improve your results.\n",
    "\n",
    "You may also search for open-source solutions and reproduce their results provided that you do **ALL** of the followings:\n",
    "\n",
    "- Properly cite and reference any source or website you've consulted with.\n",
    "\n",
    "- Only use SVMs for this part of the assignment.\n",
    "\n",
    "- Include a section in your notebook written by yourself for fine-tuning the SVM parameters to improve the available benchmark results of CIFAR-10.\n",
    "\n",
    "You should include all of the PART II code in THIS notebook.\n",
    "\n",
    "If your model meets **ALL** of the requirements above, at my discretion and judgement of your work, you may be rewarded with extra credit equivalent to up to 50% of what Assignment-2 is worth in the final weighting/grading of the Assignment part of your final grade. This could be an opportunity to reserve some points for the points you may lose in your future assignments. Assignment-2 extra credit **can't** be used to compensate for other parts of your grade such as quiz or exams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "For assignment 2, your notebook will be run and graded manually with a maximum of 100 points. Make sure that you get the correct outputs for all cells that you implement. Also, your notebook should be written with no grammatical and spelling errors and should be nicely-formatted and easy-to-read.\n",
    "\n",
    "The breakdown of the 100 points is as follows:\n",
    "\n",
    "Part I implementaion has 75 points:\n",
    "- 15 points per SGD and Logistic Regression (total 30 points): correct implementation for each classifier and getting correct score results\n",
    "- 20 points: correct SVM implementation and results\n",
    "- 25 points: correct plot of C vs f1_score\n",
    "\n",
    "Part I questions have 15 points (5 points each).\n",
    "\n",
    "The remaining 10 points will be based on your writing and formatting as instructed in the notebook.  Follow the instructions of each section carefully. Points will be deducted if your submitted notebook is not easy to read and follow or if it has grammatical and spelling errors.\n",
    "\n",
    "Part II is optional and is an extra credit opportunity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Submit and Due Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name your notebook ```Lastname-A2.ipynb```.  So, for me it would be ```Vafaei-A2.ipynb```.  Submit the file using the ```Assignment-2``` link on Blackboard.\n",
    "\n",
    "Grading will be based on \n",
    "\n",
    "  * correct behavior of the required functions, correct answer to the questions, and\n",
    "  * readability of the notebook.\n",
    "  \n",
    "<font color=red><b>Due Date: Monday Oct 14th 11:59PM.</b></font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
